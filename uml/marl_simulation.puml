@startuml OnlineRetailDynamicPricing

' Main classes with their core attributes and methods
class MarketEnv {
  - agents : list
  - current_week : int
  - current_year : int
  - is_holiday_season : bool
  - history : list
  - demand_model
  + __init__(agents, model_path)
  + step() : dict, dict
  + get_market_observations() : dict
}

class Product {
  - name : string
  - price : float
  - category_cluster : int
  - country_code : int
  - cost : float
  - quantity_history : list
  + __init__(name, price, category_cluster, country_code, cost)
  + set_price(price)
  + get_profit(demand) : float
}

abstract class PricingAgent {
  - agent_id : string
  - products : dict
  - revenue_history : list
  + __init__(agent_id, products)
  + observe(market_observations)
  + act(week, year, is_holiday, observations)
  + add_revenue(revenue)
  + calculate_total_revenue() : float
}

class SimpleAgent {
  - strategy : string
  + __init__(agent_id, products, strategy="random")
  + act(week, year, is_holiday, market_observations)
}

class LearningAgent {
  - strategy : string
  - learning_rate : float 
  - price_history : dict
  + __init__(agent_id, products, strategy="competitive", learning_rate=0.1)
  + act(week, year, is_holiday, market_observations)
}

class RLAgent {
  - learning_rate : float
  - exploration_rate : float
  - discount_factor : float
  - exploration_decay : float
  - min_exploration : float
  - q_table : dict
  - state_size : int
  - price_change_options : array
  - num_actions : int
  - memory : deque
  - profit_history : list
  + __init__(agent_id, products, learning_params)
  + observe(market_observations)
  + act(week, year, is_holiday, observations)
  + learn(reward)
  + discretize_state(state)
  + get_q_value(state, action)
  + update_q_value(state, action, reward, next_state)
  + choose_action(state)
  + get_state(product_name, market_observations)
  + remember(state, action, reward, next_state)
  + replay(batch_size)
}

class ActorNetwork {
  - fc1 : Dense
  - fc2 : Dense
  - fc3 : Dense
  - out : Dense
  + __init__(state_dim, action_dim, name)
  + call(state) : tensor
}

class CriticNetwork {
  - state_fc : Dense
  - action_fc : Dense
  - concat : Concatenate
  - fc1 : Dense
  - fc2 : Dense
  - out : Dense
  + __init__(state_dim, action_dim, name)
  + call(state, action) : tensor
}

class ReplayBuffer {
  - buffer : deque
  - max_size : int
  + __init__(max_size)
  + add(state, action, reward, next_state, done)
  + sample(batch_size) : tuple
  + size() : int
}

class MADDPGAgent {
  - actor : ActorNetwork
  - critic : CriticNetwork
  - target_actor : ActorNetwork
  - target_critic : ActorNetwork
  - buffer : ReplayBuffer
  - actor_optimizer
  - critic_optimizer
  - exploration_noise : float
  - noise_decay : float
  - min_noise : float
  - state_dim : int
  - action_dim : int
  - batch_size : int
  - tau : float
  - discount_factor : float
  - profit_history : list
  + __init__(agent_id, products, learning_params)
  + observe(market_observations)
  + act(week, year, is_holiday, observations)
  + update_networks(states, actions, rewards, next_states)
  + update_target_networks()
  + save()
  + load()
}

class EvaluationMetrics {
  + calculate_price_stability(price_df, episode_num, window)
  + nash_equilibrium_proximity(price_df, last_n_weeks)
  + revenue_optimality_gap(returns, window)
  + price_revenue_elasticity(price_df, returns)
  + global_vs_individual_optimality(price_df, returns, last_n_episodes)
  + social_welfare_metric(price_df, returns, last_n_episodes)
}

' Main simulation component
class Simulator {
  + run_simulation(weeks, episodes, num_agents, use_maddpg)
  - create_price_tracking_visualizations(price_df, changes_df)
  - save_learning_progress(episode_returns)
}

' Relationships between classes
MarketEnv o-- "*" PricingAgent : contains >
PricingAgent o-- "*" Product : owns >

PricingAgent <|-- SimpleAgent : extends
PricingAgent <|-- LearningAgent : extends
PricingAgent <|-- RLAgent : extends
PricingAgent <|-- MADDPGAgent : extends

MADDPGAgent *-- ActorNetwork : uses >
MADDPGAgent *-- CriticNetwork : uses >
MADDPGAgent *-- ReplayBuffer : uses >

Simulator --> MarketEnv : creates
Simulator --> PricingAgent : creates
Simulator --> EvaluationMetrics : uses

@enduml